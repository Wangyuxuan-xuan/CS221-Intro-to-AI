# Machine Learning 3 - Generalization, K-means
---
True goal for machine learning

![](2024-03-16-13-01-20.png)
Is to minimize error on unseen future examples

# Generalization

![](2024-03-16-13-49-06.png)
## Evealuation & Prevent overfitting
![](2024-03-16-13-51-22.png)

![](2024-03-16-14-00-09.png)
f*: The ture function/answer, the function that always get he right answer
g: The best prefictor you can get in your hypothesis calss (The fuctions u get can after feature engineering)


It is quite a trade-off

![](2024-03-16-14-02-01.png)

## How to control the size of Hypothesis class
![](2024-03-16-14-03-51.png)

### Norm

Let say we have a linear regression y = wx
The whole function space would be all linears lines that pass the origin

If somehow we can define, let's say the w is smaller or closer to 0, then we can eliminate many useless function, and thus reduce the norm
![](2024-03-16-14-10-11.png)

This means shrink the total number of weight vectors that you are considering, as u putting more constraints, they will be smaller

#### Regularization
Controlling the norm
![](2024-03-16-14-26-23.png)
You can add some `panelty` to your Traning loss function

This is saying: Optimizer, you should try to make trianing loss small, but also try to make the second part(weight vectors) small as well
 
So this is saying that, fit the data but not at the costs of having huge weight vectors(Which may be overfitting)

It is important that these need to be the same w and you are optimizing the sum

#### How to do it
Gradient descent

### Controlling the Norm - early stopping
Just stop early
![](2024-03-16-14-31-04.png)

Because normaly we initialize the weights from a baseline near 0,
For neural nets, we might give it a random value near 0

IF u have a pre-given weights, then basically ur saying don't go too far from ur initialization

We start the norm/weight vector from 0, and for each training, generally the norm gonna goes up.
So if you stop earlier, you won't give norm much chance to grow too big

### Hyperparameters
How to choose them?
![](2024-03-16-14-41-11.png)


![](2024-03-16-14-42-53.png)

## Normal Training Process
![](2024-03-16-15-08-43.png)

You can start with quite specific features and then generalize it


# Unsupervised learning
Fully labaled data is very expensive to obtain

![](2024-03-17-10-19-43.png)

## Clustering


![](2024-03-17-10-22-43.png)
z1 ... zk
Tells me which of these K clusters I'm in


### 
![](2024-03-17-10-23-54.png)

For each cluster, there's ginna be a centriod (质量重心)

The Objective function
For each point I meature the distance between that point and the centriod associated
And try to make this number as smaller as possible
![](2024-03-17-10-32-47.png)

![](2024-03-17-10-32-21.png)
A simple exapmle, you compare the distance to different centriods, and choose the closet one

![](2024-03-17-10-36-50.png)
Now it's like the chicken egg problem
IF i know the centriod, then I can do assignments
IF i know assignmemts, then I can easily figure out the centriod

But how to get those two?

### K-mean Algo

`K` is  a hyperparameter
How to u choose K?
You can try different K and choose based on loss


For a fixed itration of T
![](2024-03-17-10-44-50.png)

#### Step 1 Given Centriod and figure out the Cluster Assignments
![](2024-03-17-10-42-04.png)
Just check the distance 
you compare the distance to different centriods, and assign the point to the closet one

#### Step 2 Given Cluster assignments and try to find the centroid
![](2024-03-17-10-39-51.png)
Mathmatically you sum up all the points (feature vectors)
, and devided by the number of points => You get the centroid now!


#### Limitation
![](2024-03-17-10-51-20.png)
This is guaranteed to converged to a local minimum

K-means++ 
Initialize points far away from each other, generally works well

# Summary
![](2024-03-17-10-56-59.png)
![](2024-03-17-11-00-18.png)