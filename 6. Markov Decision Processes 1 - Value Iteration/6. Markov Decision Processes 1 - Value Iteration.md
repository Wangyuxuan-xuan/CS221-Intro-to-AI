## 6. Markov Decision Processes 1 - Value Iteration
---
## Intro
![](images/2024-03-19-13-57-10.png)
There are lot of uncertainties

![](images/2024-03-19-13-57-37.png)

We talked about search problems, where everything was deterministic
The solution was a sequence of actions
![](images/2024-03-19-14-03-07.png)

And now we are talking about this next class of state-based functions, Markov Decision Processes
The idea is that, we take actions, but we may not end up where we expected to, because there's a nature around you and there is going to be uncertainty and stuff that u did not expect
![](images/2024-03-19-14-03-52.png)
![](images/2024-03-19-14-09-02.png)

![](images/2024-03-19-14-12-08.png)

## The dice game
![](images/2024-03-19-14-19-36.png)

### Rewards
![](images/2024-03-19-14-21-40.png)
![](images/2024-03-19-14-21-50.png)

### Modeling - Markov Decision Process(MDP)
![](images/2024-03-19-14-24-32.png)
State: You can either in or out(end of the gane)
Blue: States
Red: "Chance Node", state with actions, flagging the state that ur in when taking that action
From the "Chance Node", we are going to take probobilities
![](images/2024-03-19-14-26-51.png)

![](images/2024-03-19-14-31-48.png)
Discount factor: How much you value the future

#### Different from search
![](images/2024-03-19-14-36-41.png)

About the goal:
In search problems, we want to minimize the cost
In Markov Decision Processes, we wanna maximize the reward, similar to real world view :)

![](images/2024-03-19-14-38-58.png)
What sum up to 1?

The different S' you are going to end up at, when you choose the same action
![](images/2024-03-19-15-13-30.png)

## Tram Problem Example
![](images/2024-03-19-15-16-31.png)
If the tram fails, we just gonna wast 2 minutes
reward for walk: -1
reward for tram: -2

![](images/2024-03-19-15-28-33.png)

## Policy
The policy is the soltion to MDP problem(but not guerenteed to be the most optimistic solution).

So with a give state, there are many actions we can take, which means many policies, but they are not  guaranteed to be optimistic, we have to evaluate those policies and find the best policy via policy evaluation 
In summary : Policies are like recipes, and we need to taste-test them (evaluate) to find the most satisfying one!

Policies are action to take!

![](images/2024-03-22-17-24-38.png)

### Find a solution
The solution for search problem was a sqeuence of paths, but that's when everything is deterministic
But in MDP the way we are defining the solution is using the notion of policy

The policy is a path/action to take, but it is not gueranteed to be optimistic, we have to evaluate it 

The policy is a function that takes any state and returns an action we can take?
Pi(s)  => Action
![](images/2024-03-19-15-34-47.png)

# Policy Evaluation
So ideally I would like to find the best policy to give me the best/right solution
But first, we need to tell how good a policy is, and that is the `Policy Evaluation`

A policy from state S is denoted by pi(S)
## Utility
![](images/2024-03-19-15-43-21.png)

The utility is just random variables and we cannot optimize on that
Utility is a sum of rewards, we are summing them up with discount
so Utility a **discounted sum of rewards **

![](images/2024-03-19-15-50-14.png)

And the value is the expected utility

The value of a policy is the expected utility of that policy
And that will be a number

So everytime we run under some policy, we gonnd get different form of untility (becasue of the probobility)
But the value won't change
Because it is the expectation of all untilities(statictically)

##  Discounting
The idea of this discount factor is that:
I might care about the future differently from how much I care about it now
![](images/2024-03-19-16-13-41.png)

So if gamma is 1, then the value in the future is the same as it is now
IF gamma is 0, then we don't care about the future at all, we live at the moment(greedy)
So in reliaty we'll be somewhere in between

Gamma is not really a hyperprameter that we need to tune, we shall choose the the one that fit for the problem

## Policy Evaluation
![](images/2024-03-19-16-27-37.png)
Value is the expected utility from actual state S, IF we take policy pi(s).
When we take policy pi(s), we will endup at a chance node Qpi(S,a), where the state is in between and the probility is gonna come to play and decide the actual state you are gonna reach

The Q value is expected utilities from the chance node
![](images/2024-03-19-16-31-28.png)
After the nature/probility is played, you are in a new state S', the value of the new state is `Vpi(S')`


So, what is Vpi(s) equal to 
![](images/2024-03-19-16-34-43.png)
- Gonna be 0 if I am in a end state
- OR Qpi becase i already take action a

![](images/2024-03-19-16-40-27.png)

Qpi(S,a) is the sum of probility of state S' that we could endup at * the utility of this state
```
Transition(probility) * [immidiate reward of taking this action to this state + discounted value(value of new state/future)]
```
Qpi(S,a) is the sum of expectations where all the places that I can end up at
![](images/2024-03-19-16-46-09.png)

Let's say somebody comes in and tell me the policy is to stay
![](images/2024-03-19-16-50-19.png)
![](images/2024-03-19-16-51-33.png)

## Itratively find the value
In the previous example we just solve the Vpi(S) from the equiation  (By canceling the same state)
However in the reality there can be much more and more states, and we cannot just solve from equation
 
In this case we can itratively try to find the value Vpi(S)
![](images/2024-03-19-16-58-50.png)

We initialize the values to some numbers close to 0
And we interate some number of times
For every state, we will update the value with that formula, the same equation with the value of the previous step/itration ?


This allows us to compute new values based on the previous values that I've had
I start with close to 0, and update values of all states and keep going
itrative updating, and converged to true value
![](images/2024-03-19-17-04-45.png)

## How long we should run this?
We can keep track of the difference between the value of previous time step and this time step
If that is below some threshold, then we can call it done and believe the right value has been found

![](images/2024-03-21-13-34-46.png)

We are looking for the diff between iteration t and t-1
and taking max for all possible states, becasue i want the value to be close for all states
![](images/2024-03-21-13-37-14.png)
Only store the last two columns of the table to reduce space complexity

## Complexity

![](images/2024-03-21-13-38-44.png)
 Worst case: S' = S


# Value Itration
To find the right policy
![](images/2024-03-21-13-48-11.png)

![](images/2024-03-21-13-52-47.png)

![](images/2024-03-21-13-53-53.png)
Just pick the maximun of Qopt

![](images/2024-03-21-13-56-01.png)

what is Argmax ?
>maxf(x)
 is the maximum value (if it exists) of f(x)
 as x
 varies through some domain, while argmaxf(x)
 is the value of x
 at which this maximum is attained(达到).

However, there could be more than one x
 value which gives rise to the maximum f(x)
, in which case argmaxf(x)
 would be this set of values of x
 instead.
 简而言之， 最大值和达到最大值时的自变量x值，自变量x可能不唯一

![](images/2024-03-21-13-58-06.png)

![](images/2024-03-21-13-58-56.png)

## Complexity
![](images/2024-03-21-13-59-41.png)

## Convergence
![](images/2024-03-21-15-33-36.png)
If there's no cycles, ur just doing dynamic programming, it's gonna converge, ur gonna find the optimistic one

However if u have cycles, you should make ur discount rate < 1, otherwise it will never converge (Cuz you will just keep going between cycles)

![](images/2024-03-21-15-36-48.png)

# Code
```python
from enum import Enum
import os
import sys
from queue import PriorityQueue

sys.setrecursionlimit(100000)

Destination = 10

### Model (Search Problem)
class TransportationProblemMDP(object):

    WALK_REWARD = -1
    TRAM_REWARD = -2

    WALK = "walk"
    TRAM = "tram"

    TRAM_FAIL_PROB = 0.5
    def __init__(self, destination):
        # N number of blocks
        self.destination = destination

    def startState(self) -> int:
        return 1
    
    def isEnd(self, state):
        return state == self.destination
    
    def actions(self, state):
        '''
        Return a list of valid actions that we can take form the current state
        '''
        result = []
        if(state + 1 <= self.destination):
            result.append(self.WALK)
        
        if(state * 2 <= self.destination):
            result.append(self.TRAM)

        return result
    

    def succProbAndReward(self, state : int, action: str):
        '''
        Input: 
            current state
            Action that we gonna take
        Return a list of (newState, prob, rewards) triples
        Meaning return the a list of: 
            the new state we are gonna endup at, 
            what probability of it, 
            and the reward we get from it
        
        state = s, action = a, newState = s'
        prob = T(s, a, s'), reward = Reward(s, a, s')
        '''
        result = []
        if(action == self.WALK):
            # It is deterministic, so the prob is 1.
            result.append((state + 1, 1., self.WALK_REWARD))
        elif(action == self.TRAM):
            # There's a 50% chance that the tram could fail
            result.append((state, self.TRAM_FAIL_PROB, self.TRAM_REWARD))
            result.append((state * 2, 1. - self.TRAM_FAIL_PROB, self.TRAM_REWARD))
            
        return result

    def discountFactor(self):
        return 1.
    
    def states(self):
        return range(1, self.destination+1)

# Inference (Algorithms)
def valueIteration(mdp : TransportationProblemMDP):
    # Initialize
    valueDic = {} # state => Vopt[state]
    # Initialize all optimal value with 0
    for state in mdp.states():
        valueDic[state] = 0.
    
    def VQ(state, action):
        return sum(
            transProb * (reward + mdp.discountFactor() * valueDic[newState])
            for newState, transProb, reward in mdp.succProbAndReward(state, action)
        )

    def isConverged(valueDic, newValueDic):
        '''
        We want the maximum value difference of all states to be smaller than the treashold
        '''
        return max(abs(valueDic[state] - newValueDic[state])  for state in mdp.states()) < 1e-10
            

    while True:
        # Compute the new values (new V) with old values (V)
        newValueDic = {}

        for state in mdp.states():
            if mdp.isEnd(state):
                newValueDic[state] = 0
                break

            newValueDic[state] = max(
                VQ(state, action)   
                for action in mdp.actions(state)
            )

        # Check for convergence
        if (isConverged(valueDic, newValueDic)):
            break

        valueDic = newValueDic

        # Read out policy:
        pi = {}
        for state in mdp.states():
            if mdp.isEnd(state):
                pi[state] = "none"
            else:
                # Otherwise it's gonna be the argmax of Q values
                # Which is the input that can maximize Q values
                # And how we gonna find the argmax?
                #  - We just gonna try out different actions and see
                pi[state] = max(
                    (VQ(state, action), action) for action in mdp.actions(state)
                )[1]

        # Print stuff out
        os.system('clear')
        print('{:25} {:25} {:25}'.format('state', 'valueDic[state]', 'pi[state])'))

        for state in mdp.states():
            print('{:5} {:15} {:55}'.format(state, valueDic[state], pi[state]))
        input()


mdp = TransportationProblemMDP(destination=10)
# print(mdp.actions(3))
# print(mdp.succProbAndReward(3, "tram"))
valueIteration(mdp)
```